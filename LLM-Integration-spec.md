Technischer Architekturbericht: AI Integration Plan für IMAP CampaignWizardExecutive SummaryDieser Bericht definiert die umfassende Strategie zur Integration von Google Gemini 2.5 Flash in das interne Enterprise-Tool "IMAP CampaignWizard". Ziel der Architektur ist die Automatisierung der Google Ads-Erstellung unter strikter Einhaltung der Plattform-Constraints und Nutzung eines modernen, hochperformanten Technologie-Stacks, bestehend aus Python 3.12, Litestar, Msgspec und ArangoDB.Die Kernherausforderung dieses Projekts liegt in der Symbiose aus deterministischer Softwarearchitektur (Typensicherheit, Datenbank-Konsistenz) und der probabilistischen Natur generativer KI. Während herkömmliche Web-Frameworks deterministische Inputs und Outputs erwarten, führt die Integration von Large Language Models (LLMs) Varianz ein. Um diese Varianz zu kontrollieren, setzt dieser Plan auf das "Controlled Generation"-Paradigma von Gemini 2.5 Flash, gekoppelt mit einer strikten Validierungsschicht basierend auf Msgspec.Der Bericht detailliert den Migrationspfad vom klassischen REST-Paradigma hin zu einer KI-gestützten Service-Architektur. Besondere Schwerpunkte liegen auf der Migration zum neuen google-genai SDK (v2), der Implementierung einer "Schema Bridge" zwischen Msgspec und JSON Schema sowie der Modellierung komplexer Kampagnenstrukturen als Graphen in ArangoDB. Durch die Nutzung der asynchronen Fähigkeiten von Python 3.12 und Litestar wird sichergestellt, dass die Anwendung auch bei hohen Lastspitzen und parallelen Inferenz-Anfragen reaktionsfähig bleibt.1. Technologische Fundierung und Stack-SynergienDie Auswahl des Technologie-Stacks für den "IMAP CampaignWizard" ist nicht zufällig, sondern reflektiert die Anforderungen an moderne Hochleistungsanwendungen im Enterprise-Umfeld. Jede Komponente – Python 3.12, Litestar, Msgspec und ArangoDB – spielt eine spezifische Rolle bei der effizienten Integration von Gemini 2.5 Flash.1.1 Python 3.12: Asynchrone Performance und TypensicherheitDie Basis der Anwendung bildet Python 3.12. Diese Version bringt signifikante Verbesserungen für die asynchrone Programmierung (asyncio), welche für die Interaktion mit KI-APIs essenziell ist. Da LLM-Aufrufe netzwerklastig sind und eine spürbare "Time-to-First-Token" (TTFT) aufweisen, darf der Haupt-Thread der Anwendung keinesfalls blockiert werden. Python 3.12 optimiert den Event Loop Overhead, was die Durchsatzrate bei parallelen API-Calls an Google Vertex AI oder AI Studio maximiert.Ein weiterer kritischer Aspekt ist das Typensystem. Python 3.12 verbessert die Unterstützung für Generics und Type Hints. Dies ist fundamental für die Integration mit Msgspec und der KI. Wir definieren unsere Datenmodelle nicht als lose Dictionaries, sondern als typisierte Strukturen. Die KI-Integration verlässt sich darauf, dass diese Typen zur Laufzeit validiert werden können. Die Syntax-Verbesserungen in Python 3.12 erlauben eine prägnantere Definition dieser Modelle, was die Wartbarkeit des Codes im Enterprise-Kontext erhöht.11.2 Litestar: Architektur jenseits von FastAPIDie Entscheidung für Litestar anstelle des weit verbreiteten FastAPI ist strategisch bedeutsam für die KI-Integration. Während FastAPI stark auf Pydantic und globale Zustände setzt, bietet Litestar einen objektorientierten Ansatz mit einem expliziten Dependency Injection (DI) System.Warum Litestar für AI-Integration?In vielen KI-Anwendungen wird der Client (z.B. der Google GenAI Client) oft ineffizient bei jedem Request neu instanziiert oder als globale Variable gehalten. Litestar erzwingt durch sein DI-System (Provide-Klasse, dependencies-Dictionary) eine saubere Trennung von Konfiguration und Logik. Der Gemini-Client kann als Singleton-Service injiziert werden, der Verbindungs-Pooling und Authentifizierung zentral verwaltet. Dies reduziert den Overhead pro Request signifikant und verhindert "Socket Exhaustion" bei massiven Batch-Verarbeitungen von Keywords.1Zusätzlich ist Litestars natives Verständnis von Msgspec ein entscheidender Performance-Vorteil. Da die Serialisierung und Deserialisierung von JSON-Payloads (sowohl vom Frontend als auch von der KI) CPU-intensiv sein kann, sorgt Litestars enge Integration mit Msgspec dafür, dass dieser Layer nicht zum Flaschenhals wird.41.3 Msgspec: Hochgeschwindigkeits-Serialisierung und Schema-DefinitionMsgspec ist das Herzstück der Datenvalidierung in dieser Architektur. Im Gegensatz zu Pydantic, das einen signifikanten Overhead durch komplexe Validierungslogik in Python erzeugt, ist Msgspec in C geschrieben und extrem performant.Für die Gemini-Integration stellt Msgspec jedoch eine Herausforderung dar: Die meisten Google-Beispiele und SDK-Helper nutzen Pydantic, um JSON Schemas für "Structured Outputs" zu generieren. Msgspec Structs sind zwar effizienter, werden aber von den High-Level-Wrappern der Google-SDKs oft nicht nativ unterstützt.Dieser Plan beinhaltet daher die Entwicklung einer spezialisierten "Schema Bridge". Diese Komponente nutzt msgspec.json.schema(), um aus den internen Structs kompatible JSON-Schemata zu generieren, die von Gemini 2.5 Flash verstanden werden. Dies ermöglicht es, die Performance von Msgspec beizubehalten, ohne auf die "Controlled Generation"-Features der KI verzichten zu müssen.5Vergleich der Validierungs-Frameworks im Kontext von Gemini:FeaturePydantic (Standard)Msgspec (CampaignWizard)Implikation für ArchitekturGeschwindigkeitMittel (Python-basiert)Extrem Hoch (C-Extension)Msgspec ermöglicht schnellere Verarbeitung großer Keyword-Listen.SpeicherverbrauchHoch (Objekt-Overhead)Niedrig (Struct-basiert)Wichtig bei paralleler Generierung tausender AdGroups.Schema-ExportNativ unterstützt in GenAI SDKErfordert Custom BridgeImplementierungsaufwand für Adapter notwendig.Litestar IntegrationVia PluginNativ (First-Class)Nahtlose Integration in Request/Response-Flow.1.4 ArangoDB: Multi-Model Persistenz für Ad-HierarchienGoogle Ads Daten sind inhärent hierarchisch und relational vernetzt:Ein Account hat viele Campaigns.Eine Campaign hat viele AdGroups.Eine AdGroup hat viele Ads und Keywords.Relationale Datenbanken (SQL) erzwingen starre Schemata und komplexe Joins. Dokumenten-Datenbanken (MongoDB) erschweren tiefe Verknüpfungen (z.B. "Welche Keywords performen kampagnenübergreifend am besten?"). ArangoDB als native Multi-Model-Datenbank löst dieses Dilemma.Wir modellieren die KI-generierten Kampagnen als Graphen. Die Entitäten (Kampagnen, Anzeigengruppen, Anzeigen) werden als Dokumente (Vertices) gespeichert, ihre Beziehungen als Kanten (Edges). Dies erlaubt nicht nur das einfache Speichern tief verschachtelter JSON-Strukturen, die von der KI geliefert werden, sondern auch effiziente Graph-Traversals mittels AQL (Arango Query Language). Die KI generiert im Wesentlichen Teilgraphen, die dann in den Gesamtgraphen des Unternehmens "eingehängt" werden.82. Die KI-Engine: Integration von Gemini 2.5 FlashDie Wahl des Modells und der Integrationsmethode ist entscheidend für den Erfolg des Tools. Gemini 2.5 Flash wurde spezifiziert und ist für diesen Anwendungsfall ideal geeignet.2.1 Modell-Charakteristika und EignungGemini 2.5 Flash ist ein auf Latenz und Kosteneffizienz optimiertes Modell innerhalb der Gemini-Familie. Für den "CampaignWizard" ist dies relevanter als die maximale Reasoning-Kapazität eines "Pro"-Modells.Geschwindigkeit: Die Erstellung von Kampagnen ist oft ein interaktiver Prozess. Nutzer warten ungern 30 Sekunden auf fünf Headlines. Flash bietet hier deutlich bessere Antwortzeiten.Kontext-Fenster: Mit einem Kontext von bis zu 1 Million Token kann Flash problemlos umfangreiche Landingpage-Inhalte, Produktkataloge oder historische Performance-Daten als Kontext aufnehmen, um relevantere Anzeigen zu generieren.11Kosten: Da automatisierte Kampagnenerstellung Tausende von API-Calls generieren kann (z.B. 1 Call pro Keyword-Cluster), ist das Preismodell von Flash essenziell für die Wirtschaftlichkeit des internen Tools.2.2 SDK-Strategie: Migration zu google-genaiEin kritischer Punkt, der in der aktuellen Entwicklung oft übersehen wird, ist die Fragmentierung der Google Python SDKs. Es existieren derzeit drei relevante Bibliotheken:google-cloud-aiplatform: Das klassische Vertex AI SDK.google-generativeai: Das SDK für Google AI Studio (Gen 1).google-genai: Das neue, vereinheitlichte SDK (Gen 2).12Entscheidung: Wir setzen strikt auf google-genai.Die Recherche zeigt eindeutig, dass google-genai die Zukunft der Entwicklung darstellt. Es vereinheitlicht den Zugriff auf Vertex AI und AI Studio unter einem Interface. Dies eliminiert technischen Schulden, die entstehen würden, wenn man jetzt noch auf die als "deprecated" markierten Pakete setzt. Zudem bietet das neue SDK eine verbesserte Unterstützung für moderne Python-Typisierung, was perfekt mit Python 3.12 harmoniert.2Der Migrationsaufwand von alten Tutorials ist zu beachten:Instanziierung erfolgt über Client(api_key=...) statt globaler configure()-Aufrufe.Modellzugriff erfolgt über client.models.generate_content(...) statt direkter Modell-Objekte.Die Konfiguration von response_schema ist im neuen SDK robuster gelöst, erfordert aber korrekte Typendefinitionen.2.3 Authentifizierung und Sicherheit im Enterprise-KontextFür ein internes Enterprise-Tool gelten höhere Sicherheitsanforderungen als für Prototypen. API-Keys dürfen keinesfalls hardcodiert werden.Entwicklungsumgebung: Nutzung von .env-Dateien, die von Litestar beim Start geladen werden. Der API-Key wird als Environment-Variable GEMINI_API_KEY bereitgestellt.Produktion (Google Cloud): Nutzung von "Application Default Credentials" (ADC). Wenn der "CampaignWizard" auf Cloud Run oder GKE (Google Kubernetes Engine) gehostet wird, authentifiziert sich der google-genai Client automatisch über den Metadaten-Server der Infrastruktur, ohne dass explizite Keys rotieren müssen. Dies ist der "Gold Standard" für Sicherheit.123. Implementierung der "Schema Bridge": Msgspec trifft JSON SchemaDie größte technische Hürde in diesem Stack ist die Diskrepanz zwischen Msgspec (unserem internen Datenformat) und den Erwartungen der Gemini API (JSON Schema). Gemini unterstützt "Controlled Generation" (Structured Outputs), indem es die Ausgabe zwingt, einem Schema zu folgen.3.1 Das Problem: Inkompatible StandardsMsgspec ist darauf optimiert, JSON zu parsen und in C-Structs zu mappen. Es bietet eine Funktion msgspec.json.schema(), die ein JSON Schema generiert. Jedoch:Gemini unterstützt nur ein Subset des JSON Schema Standards (OpenAPI 3.0 kompatibel).Bestimmte Keywords wie title, default oder komplexe pattern-Regexes können vom Modell ignoriert werden oder zu Fehlern führen.13Msgspec nutzt oft $defs (Definitionsblöcke) für verschachtelte Strukturen. Ältere Versionen der Gemini API hatten Probleme mit referenziellen Schemata, auch wenn die Unterstützung mit Gemini 1.5 verbessert wurde.153.2 Die Lösung: Der Schema-TransformerWir implementieren einen Utility-Service, der Msgspec-Structs in Gemini-kompatible Schemata transformiert.Algorithmische Logik:Basis-Export: Generierung des vollen Schemas mittels msgspec.json.schema(struct_type).Bereinigung: Entfernen von Metadaten-Feldern, die Tokens verschwenden oder das Modell verwirren (z.B. title, interne descriptions, die nicht für den Prompt gedacht sind).Auflösung von Referenzen: Obwohl Gemini $ref und $defs mittlerweile unterstützt 15, ist es für die Stabilität ("Haluzinations-Reduktion") oft besser, einfache Strukturen zu "inlinen", also die Definition direkt an der Stelle der Verwendung einzufügen, um dem Modell den Kontext lokal bereitzustellen.Enum-Handling: Msgspec Enums (Python Enum Klasse) werden korrekt in JSON Schema enum: ["VALUE1", "VALUE2"] Listen übersetzt. Dies ist essenziell für Google Ads Felder wie AdGroupStatus (ENABLED, PAUSED) oder KeywordMatchType (BROAD, EXACT, PHRASE).16Code-Beispiel (Konzeptionell):Pythonimport msgspec
from typing import Any, Type

def prepare_schema_for_gemini(struct: Type) -> dict[str, Any]:
    # 1. Generiere das native Schema
    schema = msgspec.json.schema(struct)
    
    # 2. Post-Processing für Gemini-Kompatibilität
    # Rekursives Entfernen nicht unterstützter Keys
    # Sicherstellen, dass 'additionalProperties' korrekt gesetzt ist (oft false)
    # Mapping von Msgspec-spezifischen Typen auf JSON-Standards
    
    # 3. Validierung gegen Gemini-Constraints (z.B. Rekursionstiefe)
    return sanitize_schema(schema)
Diese "Bridge" erlaubt es uns, im gesamten Litestar-Backend bei Msgspec zu bleiben und dennoch die volle Power der strukturierten KI-Generierung zu nutzen.4. Google Ads Domain-Modellierung & Prompt EngineeringDie Generierung von Werbetexten ist kein kreativer Freifahrtschein, sondern ein Optimierungsproblem unter harten Restriktionen. Der "CampaignWizard" muss gültige Anzeigen erzeugen, die von Google nicht abgelehnt werden.4.1 Responsive Search Ads (RSA) und Asset-StrategieDas moderne Anzeigenformat bei Google ist die "Responsive Search Ad" (RSA). Eine RSA besteht nicht aus einem fixen Text, sondern aus einem Pool von Assets, die Google dynamisch kombiniert.Headlines: Bis zu 15 Varianten (Min. 3).Descriptions: Bis zu 4 Varianten (Min. 2).Die KI darf also nicht "eine Anzeige" schreiben, sondern muss "eine Sammlung von Assets" generieren. Dies erfordert eine Listen-Struktur im Output-Schema.18Prompt-Strategie für Diversität:Wir müssen das Modell instruieren, verschiedene Arten von Headlines zu generieren, um eine hohe "Ad Strength" zu erreichen:Keywords-fokussierte Headlines.Benefit-fokussierte Headlines (Vorteile).Call-to-Action (CTA) Headlines.Ein einfacher Prompt "Schreibe 15 Headlines" führt oft zu repetitiven Ergebnissen. Ein "Chain-of-Thought" Prompt ist notwendig: "Analysiere zuerst die Alleinstellungsmerkmale (USPs). Erstelle dann 5 Headlines, die das Keyword enthalten. Erstelle danach 5 Headlines, die psychologische Trigger nutzen. Erstelle zuletzt 5 direkte Handlungsaufforderungen."4.2 Die Zeichenlimit-Herausforderung: Token vs. ZeichenDies ist der häufigste Fehler bei KI-Generatoren für Ads.Headlines: Max. 30 Zeichen.Descriptions: Max. 90 Zeichen.20LLMs wie Gemini "denken" in Tokens. Ein Wort wie "Außergewöhnlich" kann 2-3 Tokens sein, aber 15 Zeichen. Das Modell kann Zeichen zählen, aber nicht perfekt.Noch komplexer wird es bei internationalen Kampagnen (CJK - Chinesisch, Japanisch, Koreanisch). Hier zählen Zeichen oft doppelt ("Double-Width"). Ein Kanji zählt als 2 Zeichen im Google Ads Limit.18Lösungsansatz:Instruktion: Der System-Prompt muss die Limits explizit nennen ("Maximal 30 Zeichen, nicht Wörter!").Schema-Constraint: Im JSON Schema kann maxLength: 30 definiert werden. Gemini 1.5 respektiert dies meistens.Post-Processing Validierung: Wir implementieren in Python eine Validierungslogik (in der GeminiService Klasse), die nach dem Empfang der Daten die Länge prüft.Wir nutzen eine Helper-Funktion calculate_display_width(str), die Unicode-Breiten berücksichtigt.Ist eine Headline zu lang, gibt es zwei Strategien:Auto-Truncate: Kürzen, wenn es nur 1-2 Zeichen sind.Retry: Die KI mit dem Fehlercode bitten, die spezifische Headline neu zu schreiben (teurer, aber bessere Qualität).4.3 Strukturierte Datenmodelle (Msgspec Definitions)Hier definieren wir die konkreten Strukturen für den "CampaignWizard".Python# Domain Models (Pseudo-Code)
import msgspec
from typing import Annotated

# Constraints via Meta-Daten für Dokumentation und potentielle Schema-Generierung
Headline = Annotated
Description = Annotated

class RSA(msgspec.Struct):
    headlines: list[Headline]
    descriptions: list
    path1: str | None = None
    path2: str | None = None

class Keyword(msgspec.Struct):
    text: str
    match_type: str # Enum: BROAD, PHRASE, EXACT

class AdGroup(msgspec.Struct):
    name: str
    keywords: list[Keyword]
    ads: list

class CampaignStructure(msgspec.Struct):
    campaign_name: str
    budget_recommendation: float
    ad_groups: list[AdGroup]
Diese Struktur wird 1:1 in das JSON Schema für Gemini übersetzt. Das Modell füllt diese Struktur aus, und Msgspec parst sie zurück. Dies garantiert Typsicherheit bis tief in die verschachtelten Listen.55. Persistenzstrategie: Graph-Modellierung in ArangoDBNachdem die KI die Struktur generiert hat, muss diese persistiert werden. ArangoDB spielt hier seine Stärke als Graph-Datenbank aus.5.1 Schema-Design: Vertices und EdgesAnstatt die gesamte Kampagne als riesiges JSON-Blob zu speichern, zerlegen wir sie in ihre atomaren Bestandteile. Dies ermöglicht granulare Analysen (z.B. "Zeige alle AdGroups, die das Keyword 'Schuhe' nutzen, egal in welcher Kampagne").Collections (Vertices):Campaigns: { _key: "c1", name: "Sommer Sale", budget: 100 }AdGroups: { _key: "ag1", name: "Damen Sneaker" }Ads: { _key: "ad1", headlines: [...], descriptions: [...] }Keywords: { _key: "kw1", text: "rote sneaker kaufen", match_type: "BROAD" }Edge Collections (Relationships):HAS_ADGROUP: { _from: "Campaigns/c1", _to: "AdGroups/ag1" }CONTAINS_AD: { _from: "AdGroups/ag1", _to: "Ads/ad1" }TARGETS_KEYWORD: { _from: "AdGroups/ag1", _to: "Keywords/kw1" }5.2 Batch-Insert Strategie mittels AQLDie KI liefert ein tief verschachteltes Objekt (CampaignStructure). Der ArangoService muss dieses Objekt traversieren und in Datenbank-Operationen übersetzen. Ein einzelner Insert pro Keyword wäre viel zu langsam (N+1 Problem). Wir nutzen AQL (Arango Query Language) für Batch-Operationen.Wir übergeben das gesamte JSON-Objekt als Parameter an eine AQL-Query. Die Datenbanklogik übernimmt das Zerlegen (Unrolling).Beispielhafter AQL-Flow:Code-Snippet// Parameter: @campaignData (Das JSON von der KI)
INSERT { name: @campaignData.campaign_name } INTO Campaigns
LET campaignId = NEW._id

FOR group IN @campaignData.ad_groups
    INSERT { name: group.name } INTO AdGroups
    LET groupId = NEW._id
    
    // Kante erstellen
    INSERT { _from: campaignId, _to: groupId } INTO HAS_ADGROUP
    
    // Keywords verarbeiten
    FOR kw IN group.keywords
        // Upsert: Keyword nur erstellen, wenn es nicht existiert
        UPSERT { text: kw.text, match_type: kw.match_type } 
        INSERT { text: kw.text, match_type: kw.match_type } 
        UPDATE {} IN Keywords
        LET kwId = NEW._id
        
        INSERT { _from: groupId, _to: kwId } INTO TARGETS_KEYWORD
Dieser Ansatz ist extrem performant, da die gesamte Logik in der Datenbank-Engine ausgeführt wird und nur ein einziger Network-Roundtrip zwischen Litestar und ArangoDB stattfindet.96. Applikationsarchitektur & Service DesignHier definieren wir, wie diese Komponenten innerhalb des Litestar-Frameworks zusammenarbeiten.6.1 Dependency Injection (DI)Litestar nutzt ein hierarchisches DI-System. Wir definieren Provider auf App-Ebene.Python# app/dependencies.py
from litestar.di import Provide
from app.services.ai import GeminiService
from app.services.db import ArangoService

async def provide_gemini_service(config: Config) -> GeminiService:
    # Singleton-Verhalten kann via lru_cache oder Service-Lifecycle gesteuert werden
    return GeminiService(api_key=config.gemini_key)

async def provide_arango_service(config: Config) -> ArangoService:
    return ArangoService(hosts=config.arango_hosts)
6.2 Controller-DesignDer Controller (CampaignController) ist der Einstiegspunkt. Er sollte keine Geschäftslogik enthalten, sondern orchestrieren.Endpoint: POST /api/v1/campaign/generateInput: CampaignRequest (Msgspec Struct: Zielgruppe, URL, Budget).Flow:Validierung des Inputs (automatisch durch Litestar/Msgspec).Aufruf gemini_service.generate_campaign(input).Validierung/Sanitisierung der KI-Antwort (Längenprüfung).Aufruf arango_service.persist_campaign(ki_output).Rückgabe der generierten IDs an das Frontend.6.3 Middleware & ObservabilityBei KI-Anwendungen ist Logging entscheidend, aber datenschutzrechtlich sensibel.Logging: Wir loggen nicht den vollen Prompt oder die Antwort, da diese PII (Personally Identifiable Information) enthalten könnten. Wir loggen Metadaten: token_count, latency_ms, model_version.Rate Limiting: Gemini 2.5 Flash hat Quotas (RPM/TPM). Litestar bietet Middleware für Rate Limiting. Wir konfigurieren diese so, dass sie unterhalb der Google-Limits greift und dem Frontend saubere 429 Too Many Requests Fehler mit Retry-After Header sendet, statt die Google-Fehler durchzureichen.7. Operational Reliability: Fehlerbehandlung und ResilienzKI-Systeme sind nicht-deterministisch und externe APIs können ausfallen.7.1 Retry-Logik mit Exponential BackoffDer google-genai Client hat eingebaute Retry-Mechanismen, aber wir sollten auf Applikationsebene explizite Kontrolle haben, besonders bei "Structured Output" Fehlern.Wenn Gemini ein JSON liefert, das (trotz Schema) nicht parsbar ist, oder logische Fehler enthält (z.B. leere Listen), greift eine tenacity-basierte Retry-Logik.Pythonfrom tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def safe_generate(self,...):
    # API Call
Dies fängt transiente Netzwerkfehler und sporadische Modell-Fehler ab.7.2 Safety Settings & Content FilteringGoogle Gemini verfügt über strikte Sicherheitsfilter (Hate Speech, Sexually Explicit, etc.). Im Werbekontext (z.B. für medizinische Produkte oder Alkohol) können diese Filter fälschlicherweise auslösen ("False Positives").Der Integrationsplan muss vorsehen, die safety_settings im GenerateContentConfig explizit zu konfigurieren. Für einen "CampaignWizard" können wir die Filter für "Hate Speech" und "Harassment" auf BLOCK_ONLY_HIGH setzen, um zu verhindern, dass harmlose Werbetexte blockiert werden, während wir dennoch Basisschutz bieten.7.3 Testing StrategieWie testet man ein probabilistisches System?VCR.py / Replay Testing: Wir zeichnen echte API-Antworten auf und nutzen diese für deterministische Unit-Tests der Parsing- und DB-Logik.Semantic Evaluation (Eval): Für Integrationstests implementieren wir einen "LLM-as-a-Judge". Ein zweiter Aufruf (oder ein anderes Modell) prüft die generierten Ads auf Qualität und Richtlinienkonformität. "Entspricht diese Anzeige den Google Ads Richtlinien?"8. Zukunfts-Roadmap und Erweiterte Features8.1 Multimodalität (Bild-Analyse)Gemini 2.5 Flash ist multimodal. Ein zukünftiges Feature des "CampaignWizard" könnte sein: Der Nutzer lädt einen Screenshot der Landingpage hoch. Das System extrahiert visuelle Elemente (Farben, Stimmung, abgebildete Personen) und passt die Text-Assets daran an ("Passend zum Bild..."). Da Litestar effizientes File-Handling unterstützt, lässt sich dies leicht in den bestehenden GeminiService integrieren, indem Bild-Bytes direkt an die API übergeben werden.118.2 Semantisches Caching mit ArangoDBUm Kosten zu sparen, implementieren wir Caching. Da Prompts selten exakt identisch sind ("Schuhe kaufen" vs "Schuhe online kaufen"), hilft ein exakter Key-Value Cache (Redis) wenig.Da ArangoDB (mit ArangoSearch) Vektor-Suche unterstützt, können wir Embeddings der Prompts speichern. Bei neuen Anfragen suchen wir nach semantisch ähnlichen früheren Generierungen. Ist die Ähnlichkeit > 95%, liefern wir das gecachte Ergebnis. Dies reduziert Latenz und API-Kosten massiv.FazitDie Integration von Gemini 2.5 Flash in den "IMAP CampaignWizard" auf Basis von Python 3.12, Litestar, Msgspec und ArangoDB ist eine hochmoderne, leistungsfähige Architektur. Sie vermeidet den Overhead generischer KI-Frameworks und setzt auf präzise, typensichere und performante Komponenten. Durch die Implementierung der "Schema Bridge" und die Nutzung der Graphen-Fähigkeiten von ArangoDB entsteht ein System, das nicht nur Texte generiert, sondern komplexe, strukturierte und valide Geschäftsobjekte, die direkt in die Google Ads Automatisierung einfließen können. Der Plan adressiert sowohl die technischen Finessen (Async IO, C-Level Serialisierung) als auch die fachlichen Notwendigkeiten (RSA-Limits, Compliance) umfassend.